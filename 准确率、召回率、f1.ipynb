{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准确率、召回率、`f1`\n",
    "***\n",
    "***\n",
    "Time: 2020-09-20\n",
    "Author: dsy\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `sklearn`中精确率，召回率，`f1`打分**源码分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_fscore_support(y_true, y_pred, *, beta=1.0, labels=None,\n",
    "                                    pos_label=1, average=None,\n",
    "                                    warn_for=('precision', 'recall',\n",
    "                                              'f-score'),\n",
    "                                    sample_weight=None,\n",
    "                                    zero_division=\"warn\"):\n",
    "    \"\"\"Compute precision, recall, F-measure and support for each class.\n",
    "    The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of\n",
    "    true positives and ``fp`` the number of false positives. The precision is\n",
    "    intuitively the ability of the classifier not to label as positive a sample\n",
    "    that is negative.\n",
    "    The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of\n",
    "    true positives and ``fn`` the number of false negatives. The recall is\n",
    "    intuitively the ability of the classifier to find all the positive samples.\n",
    "    The F-beta score can be interpreted as a weighted harmonic mean of\n",
    "    the precision and recall, where an F-beta score reaches its best\n",
    "    value at 1 and worst score at 0.\n",
    "    The F-beta score weights recall more than precision by a factor of\n",
    "    ``beta``. ``beta == 1.0`` means recall and precision are equally important.\n",
    "    The support is the number of occurrences of each class in ``y_true``.\n",
    "    If ``pos_label is None`` and in binary classification, this function\n",
    "    returns the average precision, recall and F-measure if ``average``\n",
    "    is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``.\n",
    "    Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
    "        Estimated targets as returned by a classifier.\n",
    "    beta : float, default=1.0\n",
    "        The strength of recall versus precision in the F-score.\n",
    "    labels : array-like, default=None\n",
    "        The set of labels to include when ``average != 'binary'``, and their\n",
    "        order if ``average is None``. Labels present in the data can be\n",
    "        excluded, for example to calculate a multiclass average ignoring a\n",
    "        majority negative class, while labels not present in the data will\n",
    "        result in 0 components in a macro average. For multilabel targets,\n",
    "        labels are column indices. By default, all labels in ``y_true`` and\n",
    "        ``y_pred`` are used in sorted order.\n",
    "    pos_label : str or int, default=1\n",
    "        The class to report if ``average='binary'`` and the data is binary.\n",
    "        If the data are multiclass or multilabel, this will be ignored;\n",
    "        setting ``labels=[pos_label]`` and ``average != 'binary'`` will report\n",
    "        scores for that label only.\n",
    "    average : {'binary', 'micro', 'macro', 'samples','weighted'}, \\\n",
    "            default=None\n",
    "        If ``None``, the scores for each class are returned. Otherwise, this\n",
    "        determines the type of averaging performed on the data:\n",
    "        ``'binary'``:\n",
    "            Only report results for the class specified by ``pos_label``.\n",
    "            This is applicable only if targets (``y_{true,pred}``) are binary.\n",
    "        ``'micro'``:\n",
    "            Calculate metrics globally by counting the total true positives,\n",
    "            false negatives and false positives.\n",
    "        ``'macro'``:\n",
    "            Calculate metrics for each label, and find their unweighted\n",
    "            mean.  This does not take label imbalance into account.\n",
    "        ``'weighted'``:\n",
    "            Calculate metrics for each label, and find their average weighted\n",
    "            by support (the number of true instances for each label). This\n",
    "            alters 'macro' to account for label imbalance; it can result in an\n",
    "            F-score that is not between precision and recall.\n",
    "        ``'samples'``:\n",
    "            Calculate metrics for each instance, and find their average (only\n",
    "            meaningful for multilabel classification where this differs from\n",
    "            :func:`accuracy_score`).\n",
    "    warn_for : tuple or set, for internal use\n",
    "        This determines which warnings will be made in the case that this\n",
    "        function is being used to return only one of its metrics.\n",
    "    sample_weight : array-like of shape (n_samples,), default=None\n",
    "        Sample weights.\n",
    "    zero_division : \"warn\", 0 or 1, default=\"warn\"\n",
    "        Sets the value to return when there is a zero division:\n",
    "           - recall: when there are no positive labels\n",
    "           - precision: when there are no positive predictions\n",
    "           - f-score: both\n",
    "        If set to \"warn\", this acts as 0, but warnings are also raised.\n",
    "    Returns\n",
    "    -------\n",
    "    precision : float (if average is not None) or array of float, shape =\\\n",
    "        [n_unique_labels]\n",
    "    recall : float (if average is not None) or array of float, , shape =\\\n",
    "        [n_unique_labels]\n",
    "    fbeta_score : float (if average is not None) or array of float, shape =\\\n",
    "        [n_unique_labels]\n",
    "    support : None (if average is not None) or array of int, shape =\\\n",
    "        [n_unique_labels]\n",
    "        The number of occurrences of each label in ``y_true``.\n",
    "    Notes\n",
    "    -----\n",
    "    When ``true positive + false positive == 0``, precision is undefined.\n",
    "    When ``true positive + false negative == 0``, recall is undefined.\n",
    "    In such cases, by default the metric will be set to 0, as will f-score,\n",
    "    and ``UndefinedMetricWarning`` will be raised. This behavior can be\n",
    "    modified with ``zero_division``.\n",
    "    References\n",
    "    ----------\n",
    "    .. [1] `Wikipedia entry for the Precision and recall\n",
    "           <https://en.wikipedia.org/wiki/Precision_and_recall>`_.\n",
    "    .. [2] `Wikipedia entry for the F1-score\n",
    "           <https://en.wikipedia.org/wiki/F1_score>`_.\n",
    "    .. [3] `Discriminative Methods for Multi-labeled Classification Advances\n",
    "           in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu\n",
    "           Godbole, Sunita Sarawagi\n",
    "           <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_.\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.metrics import precision_recall_fscore_support\n",
    "    >>> y_true = np.array(['cat', 'dog', 'pig', 'cat', 'dog', 'pig'])\n",
    "    >>> y_pred = np.array(['cat', 'pig', 'dog', 'cat', 'cat', 'dog'])\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average='macro')\n",
    "    (0.22..., 0.33..., 0.26..., None)\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average='micro')\n",
    "    (0.33..., 0.33..., 0.33..., None)\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    (0.22..., 0.33..., 0.26..., None)\n",
    "    It is possible to compute per-label precisions, recalls, F1-scores and\n",
    "    supports instead of averaging:\n",
    "    >>> precision_recall_fscore_support(y_true, y_pred, average=None,\n",
    "    ... labels=['pig', 'dog', 'cat'])\n",
    "    (array([0.        , 0.        , 0.66...]),\n",
    "     array([0., 0., 1.]), array([0. , 0. , 0.8]),\n",
    "     array([2, 2, 2]))\n",
    "    \"\"\"\n",
    "    _check_zero_division(zero_division)\n",
    "    if beta < 0:\n",
    "        raise ValueError(\"beta should be >=0 in the F-beta score\")\n",
    "    labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n",
    "                                    pos_label)\n",
    "\n",
    "    # Calculate tp_sum, pred_sum, true_sum ###\n",
    "    samplewise = average == 'samples'\n",
    "    MCM = multilabel_confusion_matrix(y_true, y_pred,\n",
    "                                      sample_weight=sample_weight,\n",
    "                                      labels=labels, samplewise=samplewise)\n",
    "    tp_sum = MCM[:, 1, 1]\n",
    "    pred_sum = tp_sum + MCM[:, 0, 1]\n",
    "    true_sum = tp_sum + MCM[:, 1, 0]\n",
    "\n",
    "    if average == 'micro':\n",
    "        tp_sum = np.array([tp_sum.sum()])\n",
    "        pred_sum = np.array([pred_sum.sum()])\n",
    "        true_sum = np.array([true_sum.sum()])\n",
    "\n",
    "    # Finally, we have all our sufficient statistics. Divide! #\n",
    "    beta2 = beta ** 2\n",
    "\n",
    "    # Divide, and on zero-division, set scores and/or warn according to\n",
    "    # zero_division:\n",
    "    precision = _prf_divide(tp_sum, pred_sum, 'precision',\n",
    "                            'predicted', average, warn_for, zero_division)\n",
    "    recall = _prf_divide(tp_sum, true_sum, 'recall',\n",
    "                         'true', average, warn_for, zero_division)\n",
    "\n",
    "    # warn for f-score only if zero_division is warn, it is in warn_for\n",
    "    # and BOTH prec and rec are ill-defined\n",
    "    if zero_division == \"warn\" and (\"f-score\",) == warn_for:\n",
    "        if (pred_sum[true_sum == 0] == 0).any():\n",
    "            _warn_prf(\n",
    "                average, \"true nor predicted\", 'F-score is', len(true_sum)\n",
    "            )\n",
    "\n",
    "    # if tp == 0 F will be 1 only if all predictions are zero, all labels are\n",
    "    # zero, and zero_division=1. In all other case, 0\n",
    "    if np.isposinf(beta):\n",
    "        f_score = recall\n",
    "    else:\n",
    "        denom = beta2 * precision + recall\n",
    "\n",
    "        denom[denom == 0.] = 1  # avoid division by 0\n",
    "        f_score = (1 + beta2) * precision * recall / denom\n",
    "\n",
    "    # Average the results\n",
    "    if average == 'weighted':\n",
    "        weights = true_sum\n",
    "        if weights.sum() == 0:\n",
    "            zero_division_value = np.float64(1.0)\n",
    "            if zero_division in [\"warn\", 0]:\n",
    "                zero_division_value = np.float64(0.0)\n",
    "            # precision is zero_division if there are no positive predictions\n",
    "            # recall is zero_division if there are no positive labels\n",
    "            # fscore is zero_division if all labels AND predictions are\n",
    "            # negative\n",
    "            if pred_sum.sum() == 0:\n",
    "                return (zero_division_value,\n",
    "                        zero_division_value,\n",
    "                        zero_division_value,\n",
    "                        None)\n",
    "            else:\n",
    "                return (np.float64(0.0),\n",
    "                        zero_division_value,\n",
    "                        np.float64(0.0),\n",
    "                        None)\n",
    "\n",
    "    elif average == 'samples':\n",
    "        weights = sample_weight\n",
    "    else:\n",
    "        weights = None\n",
    "\n",
    "    if average is not None:\n",
    "        assert average != 'binary' or len(precision) == 1\n",
    "        precision = np.average(precision, weights=weights)\n",
    "        recall = np.average(recall, weights=weights)\n",
    "        f_score = np.average(f_score, weights=weights)\n",
    "        true_sum = None  # return no support\n",
    "\n",
    "    return precision, recall, f_score, true_sum\n",
    "\n",
    "def _check_zero_division(zero_division):\n",
    "    if isinstance(zero_division, str) and zero_division == \"warn\":\n",
    "        return\n",
    "    elif isinstance(zero_division, (int, float)) and zero_division in [0, 1]:\n",
    "        return\n",
    "    raise ValueError('Got zero_division={0}.'\n",
    "                     ' Must be one of [\"warn\", 0, 1]'.format(zero_division))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
